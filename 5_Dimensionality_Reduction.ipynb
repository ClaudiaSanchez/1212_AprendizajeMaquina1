{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "Sometimes, before fitting a model, it is necessary to reduce the data's dimension (number of features). There are several reasons why we are interested in reducing dimensionality as a previous preprocessing step:\n",
    "\n",
    "- In most learning algorithms, the complexity depends on the number of features, d, as well as on the number of samples.\n",
    "- When one feature is decided to be unnecessary, we save the cost of analyzing it.\n",
    "When data can be explained with fewer features, we understand it better, which allows knowledge extraction.\n",
    "- When data can be represented in a few dimensions without loss of information, it can be plotted and analyzed visually.\n",
    "\n",
    "There are two main methods for reducing dimensionality:\n",
    "- Feature selection, we are interested in finding k features (k < d) that give us the most information, and we discard the others.\n",
    "- Feature extraction, we are interested in finding a new set of k features that are the result of the combination of the original ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset Selection\n",
    "Unsupervised learning: Dimensionality reduction\n",
    "\n",
    "Variable type: all\n",
    "\n",
    "In subset selection, we are interested in finding the best subset of features. There are some simple approaches:\n",
    "- Remove features with variance 0.\n",
    "- Forward selection, we start with no variables and add them one by one, at each step adding the one that increases the model performance until any further addition does not add a better result.\n",
    "- Backward selection, we start with all variables and remove them one by one, at each step removing the one that increases the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "Unsupervised learning: Dimensionality reduction\n",
    "Variable type: continuous\n",
    "\n",
    "We are interested in finding a mapping from the original d-dimensional space's inputs to a new k-dimensional space (k < d), with minimum loss of information. PCA uses a linear projection.\n",
    "Examples in 2d and 3d.\n",
    "\n",
    "<img src=\"images/5_pca.PNG\">\n",
    "\n",
    "Principal components analysis (PCA) projects the data __maximizing the variance in the new space__. The projection vectors are calculated as the eigenvectors of the covariance matrix of the data:\n",
    "- The first vector is the eigenvector with the largest eigenvalue.\n",
    "- The second vector is the eigenvector with the second largest eigenvalue.\n",
    "- Etcetera\n",
    "There are $d$ eigenvectors. The new data $D'$ is obtained projecting the original data $D$ into the first $k$ eigenvectors $V$.\n",
    "\n",
    "$ D' = D*V $\n",
    "\n",
    "<img src=\"images/5_pca_components.PNG\">\n",
    "\n",
    "An advantage of PCA is that we can measure the proportion of variance explained by the addition of each feature.\n",
    "\n",
    "<img src=\"images/5_pca_variance_explained.PNG\">\n",
    "\n",
    "If the first two principal components explain a large percentage of the variance, we can do visual analysis: we can plot the data in a two-dimensional space and search visually for structure, groups, outliers, normality, and so forth.\n",
    "\n",
    "<img src=\"images/5_pca_example.PNG\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis\n",
    "Supervised learning: Dimensionality reduction for classification\n",
    "Variable type: continuous\n",
    "\n",
    "Linear discriminant analysis (LDA) is a supervised method for dimensionality reduction for classification problems. It was designed to project the data of two classes __maximizing the difference between the centroids and minimizing the variance of groups__ in the new space. It assumes each group's data follows a multivariate normal distribution and the variances among groups are the same.\n",
    "\n",
    "<img src=\"images/5_lda.png\">\n",
    "\n",
    "Scikit learn implementation performs LDA one feature at a time, calculating the projection vector that better separates the specific class data and the rest. Then, it sorts the projection vectors based on the quality of the separation. Finally, it projects the data on the projection vectors (one per class). The dimensionality of the new data is the minimum between the number of features and the number of classes.\n",
    "\n",
    "<img src=\"images/5_lda_projection.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multidimensional Scaling\n",
    "Unsupervised learning: Visualization\n",
    "Variable type: all\n",
    "\n",
    "For N samples, we do not know the exact coordinates of the data or their dimensionality; however, we have the distances among pairs of points, ğ‘‘ğ‘–ğ‘—, for all ğ‘–,ğ‘—=1,2,â€¦,ğ‘. Multidimensional scaling (MDS) is the method for placing these data in a dimensional space, for example, two-dimensional, such that the Euclidean distance between them there is as close as possible to ğ‘‘ğ‘–ğ‘—. MDS solves an optimization problem that minimizes the following equation called stress:\n",
    "\n",
    "$ Stress = \\sqrt{ \\frac{\\sum_{i<j} (d_{ij}-\\hat{d_{ij}})^2}{\\sum_{i<j} d_{ij}^2} } $\n",
    "\n",
    "J.B. Kruskal proposed in \"Multidimensional scaling by optimizing goodness of fit o a nonmetric hypothesis (1964)\" an interpretation of stress: 20% poor fitting, 10% fair, 5% good, 2.5% excellent, and 0% perfect.\n",
    "\n",
    "MDS can be used for dimensionality reduction by calculating Euclidean distances in the d-dimensional space and giving this input to MDS, then MDS projects it to a lower-dimensional space.\n",
    "\n",
    "Note: python's implementation uses as stress $\\frac{\\sum_{i<j} (d_{ij}-\\hat{d_{ij}})^2}{2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
