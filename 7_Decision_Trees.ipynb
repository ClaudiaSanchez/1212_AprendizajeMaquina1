{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "A decision tree is a prediction model. Given a database, several logical conditionals are generated to categorize the data. The input values can be discrete or continuous.\n",
    "The decision trees are a supervised learning technique that can be applied to solve regression and classification problems.\n",
    "\n",
    "<img src=\"images/7_decisiontrees_simpsons1.png\">\n",
    "<img src=\"images/7_decisiontrees_simpsons2.png\">\n",
    "<img src=\"images/7_decisiontrees_iris.png\">\n",
    "<img src=\"images/7_decisiontrees_administrarmedicamento.png\">\n",
    "\n",
    "The elements of a decision tree are:\n",
    "- __Nodes__: They represent a logical conditional that only depends on one feature. It can be seen as the if..else sentence. There may be several nodes with the same feature.\n",
    "- __Branches__: They grow in the nodes and correspond to the values that the logical conditional can have. Each node can have two o more branches.\n",
    "- __Leaves__: They are the final nodes and correspond to the final conclusions.\n",
    "    - If it is a classification decision tree: a leaf node represents a class\n",
    "    - If it is a regression decision tree: a leaf node represents a value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID3 algorithm\n",
    "\n",
    "ID3 is an algorithm that constructs classification decision trees based on discrete inputs. It generates the nodes from top to bottom recursively. Each time that a node is generated, it evaluates the division gain and which feature must be used.\n",
    "\n",
    "<img src=\"images/7_decisiontrees_id3_equation.png\">\n",
    "<img src=\"images/7_decisiontrees_id3_ejemplo1.png\">\n",
    "<img src=\"images/7_decisiontrees_id3_ejemplo2.png\">\n",
    "<img src=\"images/7_decisiontrees_id3_ejemplo3.png\">\n",
    "<img src=\"images/7_decisiontrees_id3_ejemplo4.png\">\n",
    "<img src=\"images/7_decisiontrees_id3_ejemplo5.png\">\n",
    "<img src=\"images/7_decisiontrees_id3_ejemplo6.png\">\n",
    "<img src=\"images/7_decisiontrees_id3_ejemplo7.png\">\n",
    "<img src=\"images/7_decisiontrees_id3_ejemplo8.png\">\n",
    "<img src=\"images/7_decisiontrees_id3_ejemplo9.png\">\n",
    "<img src=\"images/7_decisiontrees_id3_ejemplo10.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART algorithm (Classification and Regression Trees)\n",
    "\n",
    "CART is an algorithm to construct decision trees for classification and regression with discrete and continuous data. It creates binary trees.\n",
    "\n",
    "The nodes are divided as following:\n",
    "- If the feature is numeric, the conditional follows the next structure:\n",
    "\n",
    "    If [feature_value] <= [value] then <NODE1>, else <NODE2>\n",
    "\n",
    "    \n",
    "- If the feature is categorical, the conditional follows the next structure:\n",
    "    \n",
    "    If [feature_value] is in {value1,value2,‚Ä¶,valuen} then <NODE1>, else <NODE2>\n",
    "\n",
    "All the possible ways for division need to be tested, including all the cut points, and peform the one with the biggest value in the gain division.\n",
    "    \n",
    "<img src=\"images/7_decisiontrees_cart_equation.png\">\n",
    "    \n",
    "__Impurity measures__\n",
    "\n",
    "- Classification: Entropy\n",
    "    \n",
    "    $H(f) = - \\sum_i f_i log_2 f_i$\n",
    "    \n",
    "- Classification: Gini impurity\n",
    "    \n",
    "    $I_G(f) = \\sum_i f_i (1-f_i) = \\sum_i (f_i-f_i^2) = \\sum_i f_i - \\sum_i f_i^2 = 1 - \\sum_i f_i^2$\n",
    "    \n",
    "    Where $i$ goes for all the class labels, this is $i={1,2,...,m}$, and $f_i$ represents the percentage of data labeled as class $i$. Its range goes from 0 to $\\approx$1, where 0 is totally pure and $\\approx$1 is very impure.\n",
    "\n",
    "- Regression: Variance\n",
    "    \n",
    "    $\\sigma^2 = \\frac{1}{n}\\sum_i (Y_i-\\hat{Y})^2$\n",
    "    \n",
    "## Feature importance\n",
    "    \n",
    "Feature importance is proportional to the impurity reduction of all nodes related to that feature. The impurity reduction $ùêºùëÖ_ùëó$ in each node $ùëó$ representing a rule can be calculated with:\n",
    "    \n",
    "$IR_j = w_jI_j - (w_{left}I_{left} + w_{right}I_{right})$\n",
    "    \n",
    "where $left$ and $right$ represent the children nodes of node $ùëó$, $ùêº$ represents the impurity of each node, and the weights $ùë§$ are the samples‚Äô proportion in nodes, and they are calculated as the number of samples in the node divided by the total number of samples. Once the impurity reduction in all nodes is known, the importance of the feature $ùëò$, $ùêπùêº_ùëò$, is calculated as following:\n",
    "    \n",
    "$FI_k = \\frac{\\sum_{j\\in N_k}IR_j}{\\sum_{j\\in N}IR_j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages and disadvantages of decision trees\n",
    "\n",
    "Advantages:\n",
    "- Easy to interpret and understand\n",
    "- Require few or null data pre-processing\n",
    "- Can handle with numerical and categorical data (sklearn version only can be executed with numerical data)\n",
    "- It is a white box model\n",
    "- Work well with big data\n",
    "\n",
    "Disadvantages:\n",
    "- They have a big problem of overfitting\n",
    "- They are based on greedy algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble\n",
    "\n",
    "Ensemble: Construction of several simple learning models, each one with the same or different training dataset. The final decision is calculated based on a votation scheme. Ensembles obtain better predictive performance that all its models by itself. They reduce the overfitting because the models are very simple.\n",
    "\n",
    "__Random Forest__ is a supervised learning model formed by several decision trees. All the trees are different among them because they are training with a randomly selected subset of samples and features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
