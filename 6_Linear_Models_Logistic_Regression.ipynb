{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "Supervised learning: classification\n",
    "Type of features: numeric\n",
    "\n",
    "Logistic regression is a model for binary classification. It consists into fitting a point (1d), a line (2d), a plane (3d) or a hyperplane (+3D) to separate the data of different classes.\n",
    "The correct name should be __logistic classification__, but the name has been keept because was the original proposal of the author.\n",
    "\n",
    "<img src=\"images/6_logistic_regression.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the models as follows:\n",
    "\n",
    "- 1D, a point:   $w_0 + w_1x = 0$\n",
    "- 2D, a line:   $w_0 + w_1x_1 + w_2 x_2 = 0$\n",
    "- 3D, a plane:   $w_0 + w_1 x_1 + w_2 x_2 + w_3 x_3 = 0$\n",
    "\n",
    "In general, a hyperpane:  $W^T X = 0$\n",
    "where $W=[w_0,w_1,...,w_n]^T$ and $X=[1,x_1,...,x_n]$\n",
    "\n",
    "The objective is to identify the class using the sign of the dot product:\n",
    "- $ W^T X > 0 \\rightarrow Class 1$\n",
    "- $ W^T X < 0 \\rightarrow Class 2$\n",
    "\n",
    "<img src=\"images/6_logistic_regression_classes.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the problem as the likelihood (probability) of belonging to a class, then we follow the next rules:\n",
    "- The classesâ€™ labels are 0 and 1.\n",
    "- The sigmoid function is used to add the probability effect.\n",
    "- The log function is used to define the optimization problem.\n",
    "\n",
    "<img src=\"images/6_logistic_regression_log.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation of the decision limit\n",
    "\n",
    "The original model is a hyperplane. However, if we transform the variables __before__ fitting the model we can have a non linear model.\n",
    "\n",
    "- $ h(x) = w_0 + w_1 x_1 + w_2 x_2 $\n",
    "- $ h(x) = w_0 + w_1 x_1 + w_2 x_2 + w_3 x_1^2 + w_4 x_2^2 $\n",
    "\n",
    "<img src=\"images/6_logistic_regression_quadratic.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass classification: one vs all\n",
    "\n",
    "Sometimes, a binary classification is not enough.\n",
    "\n",
    "Examples of binary classification:\n",
    "- Identify SPAM email\n",
    "\n",
    "Examples of multiclass classification:\n",
    "- Weather: sunny, cloudy, rainy, snowy.\n",
    "- Face expression recognition: happy, sad, surprise, angry.\n",
    "\n",
    "<img src=\"images/6_logistic_regression_multiclass.png\">\n",
    "\n",
    "One classifier is created by each class. To create the classifier, the class data are marked as 1 and the rest with 0. For classifying a new sample, it is evaluated in all the classifiers and the label assigned is the one that corresponds with the classifier where the value gets the highest probability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
