{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'> Datasets, Visualization and Metrics </font> \n",
    "\n",
    "All the Machine Learning algorithms need data. The more the data, the better the learning. The data is required for the learning process of algorithms. A dataset represents all the information that can be processed. For example:\n",
    "- 1000 images of faces, each one labeled with the person's name.\n",
    "- Database of the sales department.\n",
    "- A survey data.\n",
    "\n",
    "Traditionally, the information is preprocessed to have a table with values, where the columns and rows represent the features and samples, respectively. A **feature**, also called variable, represents a characteristic of the object. A **sample** is a concrete object. Some classic nomenclature is:\n",
    "\n",
    "$ùëÅ=$ Number of features\n",
    "\n",
    "$ùëÄ=$ Number of samples\n",
    "\n",
    "$ùë•_ùëñ=$ Feature $i$ values ($i$ goes for all the samples)\n",
    "\n",
    "$ùë•^{(ùëñ)}=$ Sample $i$ values ($i$ goes for all the features)\n",
    "\n",
    "$ùë¶^{(ùëñ)}=$ Output or label of the sample $i$\n",
    "\n",
    "$ùë•_ùëó^{(ùëñ)}=$ Sample $i$, feature $j$\n",
    "\n",
    "The following images show examples of datasets. The first one is a house sales example, and the second one a dataset that comes from a face recognition problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/2_dataset_housesales.png\">\n",
    "<img src=\"images/2_dataset_faces.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of features\n",
    "\n",
    "#### First classification\n",
    "\n",
    "- **Numeric**: Numbers, they can be integers or continuos. For example: age, price, temperature.\n",
    "- **Ordinal**: The values of these features are not numbers, but their values have a specific order. For example: How do you feel today? Very unhappy, unhappy, neutral, happy or very happy. Or scholarly: elementary school, high school, college, master, doctorate.\n",
    "- **Categorical or nominal**: The values of these features represent categories. They cannot be ordered. For example, gender, hair color, birthplace.\n",
    "\n",
    "#### Second classification\n",
    "\n",
    "- **Continuous**: A feature is considered continuous when it has an unlimited number of values. Here we can include the numeric variables.\n",
    "- **Discrete**: A feature is considered discrete when it has a limited number of values. Regularly, this number of values is small. Here we can include ordinal and categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'> Numerical features </font> \n",
    "\n",
    "<u>Some metrics that can be helpful to understand numeric features are:</u>\n",
    "\n",
    "- Minimum\n",
    "- Maximum\n",
    "- Range: $ Maximum - minimum $\n",
    "- Average or mean: $\\bar{x}=\\frac{\\sum_i x_i}{n}$\n",
    "- Median: If we order the samples, the median is the value that lies just in the middle.\n",
    "- Variance:   population $\\sigma^2=\\frac{\\sum_i (x_i-\\bar{x})^2}{n}$   sample $s^2=\\frac{\\sum_i (x_i-\\bar{x})^2}{n-1}$\n",
    "- Standard deviation:  population $\\sigma=\\sqrt{\\sigma^2}$ sample $s=\\sqrt{s^2}$. The standard deviation can be seen as the average of all the variations between the data and the average.\n",
    "\n",
    "- Quartiles: It divides the number of data points into four parts. The data must be ordered from smallest to largest. The value of 25% of the data is below to first quartile (Q1). The second quartile (Q2) is the median of a data set; thus, 50% of the data lies below this point. The value of 75% of the data is below to first quartile (Q3).\n",
    "\n",
    "- Interquartile range: It is useful to find outliers. $IQR = Q_3- Q_1$. An outlier is a sample whose value $x$ is $x<Q_1-1.5*IQR$ or $x>Q_3+1.5*IQR$.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>For measuring the dependency between two numeric features we can use covariance and correlation.</u>\n",
    "\n",
    "**_Covariance_**\n",
    "\n",
    "It is a numeric value that measures the lineal dependency between two numeric features. \n",
    "\n",
    "Population  $\\sigma_{xy}=\\frac{\\sum_i (x_i-\\bar{x})(y_i-\\bar{y})}{n}$  sample  $\\sigma_{xy}=\\frac{\\sum_i (x_i-\\bar{x})(y_i-\\bar{y})}{n-1}$\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "- A positive value indicates an increase in one variable results in an increase in the other variable. \n",
    "- A negative value indicates an increase in one variable results in the opposite change in the other variable.\n",
    "- A value close to zero indicates that there is no linear dependency. However, there could be a nonlinear dependency.\n",
    "\n",
    "[See image](https://en.wikipedia.org/wiki/Covariance#/media/File:Covariance_trends.svg)\n",
    "\n",
    "The covariance's problem is that it does not have a limited range. It is difficult to know if a covariance value of 343 represents a strong or a weak dependency.\n",
    "\n",
    "**_Correlation_**\n",
    "\n",
    "Pearson's correlation coefficient, commonly called the correlation coefficient, is a numeric value that measures the linear relationship between two numeric features. The difference with the covariance is that it has a limited range between -1 and 1.\n",
    "\n",
    "Population  $r = \\frac{\\sigma_{xy}}{\\sigma_x \\sigma_y} = \\frac{Cov(x,y)}{StdDev(x) StdDev(y)}$\n",
    "\n",
    "Sample  $r=\\frac{\\sum_i (x_i-\\bar{x})(y_i-\\bar{y})}{ \\sqrt{\\sum_i (x_i - \\bar{x})^2} \\sqrt{\\sum_i (y_i - \\bar{y})^2} }$  \n",
    "\n",
    "Interpretation:\n",
    "\n",
    "- The sign represents the same as covariance.\n",
    "- It is a number between -1 and 1\n",
    "    - The closer to -1 or 1, the more the linear dependency.\n",
    "    - Values close to 0 indicates that there is no linear dependency.\n",
    "\n",
    "[See image](https://en.wikipedia.org/wiki/Correlation_and_dependence#/media/File:Correlation_examples2.svg)\n",
    "\n",
    "From my personal perspective, it is much better to use correlation than covariance to measure the linear dependency between numerical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>The graphs that we can use to visualize numeric features are:</u>\n",
    "\n",
    "- Histogram (1 feature). It is similar to a bar plot, but the difference is that a bar plot is used for categorical features and histograms for numeric features. It means the bars' ranges represent values, they are ordered, and we can change the width or number of bars. It is very useful for recognizing the distribution of the variables or if it has one or two modes. [See examples](https://www.google.com/search?q=histograms&safe=strict&rlz=1C1SQJL_enMX896MX896&sxsrf=ALeKk01wtRX3bg52N0to6JBMentknDEFpA:1608809662185&source=lnms&tbm=isch&sa=X&ved=2ahUKEwjhxLODw-btAhURQq0KHVvKDNYQ_AUoAXoECAMQAw).\n",
    "\n",
    "- Boxplot (1 feature). It represents the distribution of a numerical feature based on quartiles. It is very useful to recognize outliers. The box width represents how sparse the samples are. [See examples](https://www.google.com/search?q=boxplot&safe=strict&rlz=1C1SQJL_enMX896MX896&sxsrf=ALeKk01yC2D3sw34EshMLIlIHz0mrQvdOQ:1608810342412&source=lnms&tbm=isch&sa=X&ved=2ahUKEwjwlOHHxebtAhUFQK0KHQFkCK0Q_AUoAXoECA8QAw).\n",
    "\n",
    "- Scatter plot (2 features). It is really useful for understand the relationship between two numerical features. [See examples](https://www.google.com/search?q=scatter+plot&safe=strict&rlz=1C1SQJL_enMX896MX896&sxsrf=ALeKk01SKMKoHvnFX1aCJjSGS3I8LiCNnQ:1608810420509&source=lnms&tbm=isch&sa=X&ved=2ahUKEwi14v_sxebtAhWOQs0KHfh3ARgQ_AUoAXoECAIQAw&biw=1093&bih=454).\n",
    "\n",
    "- Bubble plot (3 features). It is similar to a scatter plot, but using colors and sizes it can represent more than two features. [See examples](https://www.google.com/search?q=bubble+plot&tbm=isch&ved=2ahUKEwjKja3wxebtAhXXa6wKHXKTA5oQ2-cCegQIABAA&oq=bubble+plot&gs_lcp=CgNpbWcQA1DZ3wFYlukBYMTqAWgAcAB4AIABAIgBAJIBAJgBAKABAaoBC2d3cy13aXotaW1nwAEB&sclient=img&ei=u3_kX4rEINfXsQXypo7QCQ&bih=454&biw=1093&rlz=1C1SQJL_enMX896MX896&safe=strict).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'> Categorical features </font> \n",
    "\n",
    "<u>Some metrics that can be helpful to understand categorical features are:</u>\n",
    "\n",
    "- Mode. It is the value that appears most often.\n",
    "\n",
    "**_Entropy_**\n",
    "\n",
    "The entropy, also called entropy of the information or Shannon‚Äôs entropy, measures the **surprise** or the **chaos** in the values of a variable. Formally, it measures the uniformity of the variable. It can be calculated as follows:\n",
    "\n",
    "$ H(X) = - \\sum_x P(x) log_2 P(x) $\n",
    "\n",
    "<img src=\"images/2_entropy.png\">\n",
    "\n",
    "The problem is that there is no maximum value for entropy. For each variable, the maximum value is $ H(X)=-log_2\\left ( \\frac{1}{No.categories} \\right )$\n",
    "\n",
    "**_Mutual Information_**\n",
    "\n",
    "Mutual information measures how much one random variable tells us about another.\n",
    "\n",
    "$ IM(X,Y) = \\sum_x \\sum_y P(x,y) log_2 \\frac{P(x,y)}{P(x)P(y)} $\n",
    "\n",
    "If $X,Y$ are independent then $ \\forall_{x,y} P(x,y) = P(x)P(y) $\n",
    "\n",
    "<img src=\"images/2_mutualinformation.png\">\n",
    "\n",
    "\n",
    "**_Entropy and Mutual Information_**\n",
    "\n",
    "<img src=\"images/2_entropy_mutualinformation_venndiagram.png\">\n",
    "\n",
    "$ IM(A,B) = H(A) - H(A|B) $\n",
    "\n",
    "$ IM(A,B) = H(B) - H(B|A) $\n",
    "\n",
    "$ IM(A,B) = H(A) + H(B) - H(A,B) $\n",
    "\n",
    "$ IM(A,B) = H(A,B) - H(A|B) - H(B|A) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>The graphs that we can use to visualize categorical features are:</u>\n",
    "\n",
    "- Bar plot. Each bar length represents the frequency of each category. [See examples](https://www.google.com/search?q=bar+plot&tbm=isch&ved=2ahUKEwiX3_bV_NDuAhUMNK0KHaNaD6kQ2-cCegQIABAA&oq=bar+plot&gs_lcp=CgNpbWcQA1DEjgdY9JUHYPOWB2gAcAB4AIABAIgBAJIBAJgBAKABAaoBC2d3cy13aXotaW1nwAEB&sclient=img&ei=REwcYJetFIzotAWjtb3ICg&bih=454&biw=1093&rlz=1C1SQJL_enMX896MX896&safe=strict). \n",
    "\n",
    "- Pie plot. Each section represents the proportion of elements in each category. [See examples](https://www.google.com/search?q=pie+plot&safe=strict&rlz=1C1SQJL_enMX896MX896&sxsrf=ALeKk02sIRqsG9-mh48NzjqKRhFfZDPRRQ:1612467266079&source=lnms&tbm=isch&sa=X&ved=2ahUKEwiH4uzU_NDuAhVJip4KHQd7A0wQ_AUoAXoECAMQAw&biw=1093&bih=454&dpr=1.25w). Be careful because if pie plots are represented with different effects, they can cause misunderstanding. [See example](https://www.indiabix.com/data-interpretation/pie-charts/).\n",
    "\n",
    "- Rectangular pie chart. It is similar to a pie plot, but we use rectangles instead of a circle. [See examples](https://www.google.com/search?q=rectangular+pie+chart&tbm=isch&ved=2ahUKEwjgkZmk_tDuAhWGa6wKHQGfCg8Q2-cCegQIABAA&oq=rectangular+pie+chart&gs_lcp=CgNpbWcQAzIFCAAQsQMyBQgAELEDMgUIABCxAzICCAAyAggAMgIIADICCAAyAggAMgIIADICCAA6BAgjECc6BAgAEEM6CAgAELEDEIMBUIU3WKl6YL57aABwAHgAgAHcA4gB8yCSAQcyLTMuNi4zmAEAoAEBqgELZ3dzLXdpei1pbWfAAQE&sclient=img&ei=9E0cYODHN4bXsQWBvqp4&bih=454&biw=1093&rlz=1C1SQJL_enMX896MX896&safe=strict).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
