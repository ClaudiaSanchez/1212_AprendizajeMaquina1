{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVM)\n",
    "Supervised learning: Classification and regression\n",
    "Variable type: continuous\n",
    "\n",
    "## SVM for classification\n",
    "\n",
    "SVM for binary classification is a model that, like logistic regression, identifies a line (2d), a plane (3d) or a hyperplane that divide the data into two classes. Its name is because the data are considered as vectors.\n",
    "\n",
    "<img src=\"images/6_svm1.png\">\n",
    "\n",
    "There may be many models (lines) that separate the classesâ€™ vectors, but, Â¿Which one is the best?\n",
    "\n",
    "<img src=\"images/6_svm2.png\">\n",
    "\n",
    "The SVM calculates a linear classification model maximizing the margin between the classes.\n",
    "\n",
    "<img src=\"images/6_svm_margin.png\">\n",
    "\n",
    "To find the hyperplane it is necessary to design an optimization problem that maximices the margin. The proposal is as follows:\n",
    "\n",
    "<img src=\"images/6_svm_optimization.png\">\n",
    "\n",
    "A more practical implementation is (sci-kit learn):\n",
    "\n",
    "<img src=\"images/6_svm_optimization_sklearn.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernels\n",
    "\n",
    "Sometimes a linear model is not enough, like in the next esample:\n",
    "\n",
    "<img src=\"images/6_svm_kernel1.png\">\n",
    "\n",
    "The use of __kernels__ allows to transform the data from the __original vectorial space to another vectorial space__ (the kernel space) where there can exist and hyperplane that separates the classes.\n",
    "\n",
    "Intuition:\n",
    "- First, we have the data in the original space\n",
    "- Then, a landmark is added. It is in the original space\n",
    "- The distance between the data points and the landmark is measured. For example, Euclidean distance can be used.\n",
    "- Using only those distances, we can transform the points from 2-d to 1-d. In this space, the classes are linearly separable.\n",
    "\n",
    "<img src=\"images/6_svm_kernel2.png\">\n",
    "<img src=\"images/6_svm_kernel3.png\">\n",
    "\n",
    "In fact, SVM with kernels uses all the points as landmarks. In this case the vectorial space is transform from m-d to n-d. m is the number of features and m is the number of samples.\n",
    "\n",
    "<img src=\"images/6_svm_kernel_example.png\">\n",
    "\n",
    "The most popular kernel is the Gaussian kernel. The similarity function is defined as follows:\n",
    "\n",
    "$ f = similitud(x,l) = exp\\left ( -\\frac{\\left \\| x-l \\right \\|^2}{2\\sigma^2} \\right )$\n",
    "\n",
    "Where $l$ represents the landmark, $\\left \\| \\cdot  \\right \\|^2$ the squared norm. Remembering $\\left \\| U \\right \\| = \\sqrt{u_1^2 + u_2^2 + ... + u_m^2}$. $\\sigma$ can change the Gaussian's influence.\n",
    "\n",
    "<img src=\"images/6_svm_kernel4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM for regression\n",
    "\n",
    "Smola and SchÃ¶lkopf (2004) describe how can be used SVM specifically to regression problems. (Smola, A. J., & SchÃ¶lkopf, B. (2004). A tutorial on support vector regression. Statistics and computing, 14(3), 199-222.)\n",
    "\n",
    "Given $ğ‘$ input vectors $ğ‘¥_1,ğ‘¥_2,â€¦,ğ‘¥_ğ‘$ in a ğ‘‘-dimensional space $ğ‘¥_ğ‘–\\in \\mathbb{R}^ğ‘‘$, and ğ‘ output values $ğ‘¦_1,ğ‘¦_2,â€¦,ğ‘¦_ğ‘$ where $ğ‘¦_ğ‘–\\in \\mathbb{R}$, it is wanted to find the function $ğ‘“$ that minimizes the difference between $ğ‘“(ğ‘¥_ğ‘–)$ and $ğ‘¦_ğ‘–$. The model proposed by SVM to solve the previous problem is $ğ‘“(ğ‘¥_ğ‘–)=âŒ©ğ‘¤,ğ‘¥_ğ‘–âŒª+ğ‘$, where âŒ©ğ‘¢,ğ‘£âŒª represents the dot product between the vectors ğ‘¢ and ğ‘£, and the vector ğ‘¤, and the scalar value ğ‘ are the parameters of a linear model.\n",
    "To find the values of the vector ğ‘¤ and the scalar ğ‘, SVM solves an optimization problem that constructs a margin that wraps the linear model. SVM tries to minimize the distance between the margin and the value $ğ‘“(ğ‘¥_ğ‘–)$ for those input-output training data $(ğ‘¥_ğ‘–,ğ‘¦_ğ‘–)$ where $|ğ‘“(ğ‘¥_ğ‘–)âˆ’ğ‘¦_ğ‘–|â‰¥\\epsilon$. It means all the pair of input-output training where $|ğ‘“(ğ‘¥_ğ‘–)âˆ’ğ‘¦_ğ‘–|<\\epsilon$ do not contribute to the optimization function.\n",
    "\n",
    "<img src=\"images/6_svm_regression.png\">\n",
    "\n",
    "The optimization problem is defined as:\n",
    "\n",
    "$min_{w,b,\\zeta,\\zeta^*} \\frac{1}{2} \\left \\langle w,w  \\right \\rangle+ C \\sum_{i=1}^n (\\zeta_i + \\zeta^*_i)$\n",
    "\n",
    "Subject to\n",
    "\n",
    "$y_i - f(x_i) \\leqslant \\epsilon + \\zeta_i$\n",
    "\n",
    "$f(x_i)-y_i \\leqslant \\epsilon + \\zeta^*_i$\n",
    "\n",
    "$\\zeta_i, \\zeta^*_i \\geqslant 0, i=1,...,n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
