{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVM)\n",
    "Supervised learning: Classification and regression\n",
    "Variable type: continuous\n",
    "\n",
    "## SVM for classification\n",
    "\n",
    "SVM for binary classification is a model that, like logistic regression, identifies a line (2d), a plane (3d) or a hyperplane that divide the data into two classes. Its name is because the data are considered as vectors.\n",
    "\n",
    "<img src=\"images/6_svm1.png\">\n",
    "\n",
    "There may be many models (lines) that separate the classes’ vectors, but, ¿Which one is the best?\n",
    "\n",
    "<img src=\"images/6_svm2.png\">\n",
    "\n",
    "The SVM calculates a linear classification model maximizing the margin between the classes.\n",
    "\n",
    "<img src=\"images/6_svm_margin.png\">\n",
    "\n",
    "To find the hyperplane it is necessary to design an optimization problem that maximices the margin. The proposal is as follows:\n",
    "\n",
    "<img src=\"images/6_svm_optimization.png\">\n",
    "\n",
    "A more practical implementation is (sci-kit learn):\n",
    "\n",
    "<img src=\"images/6_svm_optimization_sklearn.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernels\n",
    "\n",
    "Sometimes a linear model is not enough, like in the next esample:\n",
    "\n",
    "<img src=\"images/6_svm_kernel1.png\">\n",
    "\n",
    "The use of __kernels__ allows to transform the data from the __original vectorial space to another vectorial space__ (the kernel space) where there can exist and hyperplane that separates the classes.\n",
    "\n",
    "Intuition:\n",
    "- First, we have the data in the original space\n",
    "- Then, a landmark is added. It is in the original space\n",
    "- The distance between the data points and the landmark is measured. For example, Euclidean distance can be used.\n",
    "- Using only those distances, we can transform the points from 2-d to 1-d. In this space, the classes are linearly separable.\n",
    "\n",
    "<img src=\"images/6_svm_kernel2.png\">\n",
    "<img src=\"images/6_svm_kernel3.png\">\n",
    "\n",
    "In fact, SVM with kernels uses all the points as landmarks. In this case the vectorial space is transform from m-d to n-d. m is the number of features and m is the number of samples.\n",
    "\n",
    "<img src=\"images/6_svm_kernel_example.png\">\n",
    "\n",
    "The most popular kernel is the Gaussian kernel. The similarity function is defined as follows:\n",
    "\n",
    "$ f = similitud(x,l) = exp\\left ( -\\frac{\\left \\| x-l \\right \\|^2}{2\\sigma^2} \\right )$\n",
    "\n",
    "Where $l$ represents the landmark, $\\left \\| \\cdot  \\right \\|^2$ the squared norm. Remembering $\\left \\| U \\right \\| = \\sqrt{u_1^2 + u_2^2 + ... + u_m^2}$. $\\sigma$ can change the Gaussian's influence.\n",
    "\n",
    "<img src=\"images/6_svm_kernel4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM for regression\n",
    "\n",
    "Smola and Schölkopf (2004) describe how can be used SVM specifically to regression problems. (Smola, A. J., & Schölkopf, B. (2004). A tutorial on support vector regression. Statistics and computing, 14(3), 199-222.)\n",
    "\n",
    "Given $𝑁$ input vectors $𝑥_1,𝑥_2,…,𝑥_𝑁$ in a 𝑑-dimensional space $𝑥_𝑖\\in \\mathbb{R}^𝑑$, and 𝑁 output values $𝑦_1,𝑦_2,…,𝑦_𝑁$ where $𝑦_𝑖\\in \\mathbb{R}$, it is wanted to find the function $𝑓$ that minimizes the difference between $𝑓(𝑥_𝑖)$ and $𝑦_𝑖$. The model proposed by SVM to solve the previous problem is $𝑓(𝑥_𝑖)=〈𝑤,𝑥_𝑖〉+𝑏$, where 〈𝑢,𝑣〉 represents the dot product between the vectors 𝑢 and 𝑣, and the vector 𝑤, and the scalar value 𝑏 are the parameters of a linear model.\n",
    "To find the values of the vector 𝑤 and the scalar 𝑏, SVM solves an optimization problem that constructs a margin that wraps the linear model. SVM tries to minimize the distance between the margin and the value $𝑓(𝑥_𝑖)$ for those input-output training data $(𝑥_𝑖,𝑦_𝑖)$ where $|𝑓(𝑥_𝑖)−𝑦_𝑖|≥\\epsilon$. It means all the pair of input-output training where $|𝑓(𝑥_𝑖)−𝑦_𝑖|<\\epsilon$ do not contribute to the optimization function.\n",
    "\n",
    "<img src=\"images/6_svm_regression.png\">\n",
    "\n",
    "The optimization problem is defined as:\n",
    "\n",
    "$min_{w,b,\\zeta,\\zeta^*} \\frac{1}{2} \\left \\langle w,w  \\right \\rangle+ C \\sum_{i=1}^n (\\zeta_i + \\zeta^*_i)$\n",
    "\n",
    "Subject to\n",
    "\n",
    "$y_i - f(x_i) \\leqslant \\epsilon + \\zeta_i$\n",
    "\n",
    "$f(x_i)-y_i \\leqslant \\epsilon + \\zeta^*_i$\n",
    "\n",
    "$\\zeta_i, \\zeta^*_i \\geqslant 0, i=1,...,n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
