{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "Supervised learning: regression\n",
    "Type of features: numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a linear model to predict an output variable based on one or more input variables. It could be:\n",
    "\n",
    "- __A line__. It is the case when we have 1 input and 1 output variable. [See examples](https://www.google.com/search?q=linear+regression&safe=strict&rlz=1C1SQJL_enMX896MX896&sxsrf=ALeKk01DD8SF_xTdScaocl1elApi1boLIA:1613589398191&source=lnms&tbm=isch&sa=X&ved=2ahUKEwixuJH30PHuAhVOC6wKHSFtBbIQ_AUoAXoECBsQAw&biw=1366&bih=568).\n",
    "\n",
    "$ h(X) = \\beta_1 x + \\beta_0 $\n",
    "\n",
    "For example: Grade = 0.5* hrs_study + 4 \n",
    "\n",
    "- __A plane__. It is the case when we have 2 input and 1 output variable. [See examples](https://www.google.com/search?q=linear+regression+2+input+features&tbm=isch&ved=2ahUKEwjg5YqB0fHuAhUNSKwKHSoNAXYQ2-cCegQIABAA&oq=linear+regression+2+input+features&gs_lcp=CgNpbWcQAzoECCMQJ1DIYljPZ2CaaWgAcAB4AIABbYgB8gSSAQMxLjWYAQCgAQGqAQtnd3Mtd2l6LWltZ8ABAQ&sclient=img&ei=q2stYKDkAo2QsQWqmoSwBw&bih=568&biw=1366&rlz=1C1SQJL_enMX896MX896&safe=strict).\n",
    "\n",
    "$ h(X) = \\beta_2 x_2 + \\beta_1 x_1 + \\beta_0 $\n",
    "\n",
    "For example: Sales = 0.54* TV + 3*Radio + 4\n",
    "\n",
    "- __A hyperplane__. It is the case when we have more than 2 input variables and 1 output variable.\n",
    "\n",
    "$ h(X) = \\beta_n x_n + ... + \\beta_2 x_2 + \\beta_1 x_1 + \\beta_0 $\n",
    "\n",
    "$ h(X) = \\sum_{i=1}^n \\beta_i x_i + \\beta_0 $\n",
    "\n",
    "$ X = \\left [ \\begin{matrix}\n",
    "x_1 \\\\ \n",
    "x_2 \\\\ \n",
    "\\vdots \\\\ \n",
    "x_n \\\\ \n",
    "\\end{matrix} \\right ] $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective is to minimize a cost function $J$. It consists of the difference between the real output and the prediction of the model for all the samples ([see images](https://www.google.com/search?q=linear+regression+optimization&safe=strict&rlz=1C1SQJL_enMX896MX896&sxsrf=ALeKk0165RZhp4VE3vSybU_0owFyECwaJQ:1613590618440&source=lnms&tbm=isch&sa=X&ved=2ahUKEwjJvP-81fHuAhVQIqwKHcWcBmgQ_AUoAXoECAYQAw&biw=1366&bih=568)):\n",
    "\n",
    "$ min_{\\beta} J(\\beta) = \\frac{1}{2} \\sum_{i=1}^M (h (x^{(i)}) - y^{(i)} ) ^2$\n",
    "\n",
    "Remembering, a dataset is composed by pairs of input-output data $< x^{(i)},y^{(i)}>$, where $i$ represents the sample $i$th, $x^{(i)} \\in \\mathbb{R}^n$ is a vector that contains the values of the features, and $y^{(i)} \\in \\mathbb{R}$ contains the output value.\n",
    "\n",
    "<img src=\"images/6_input_output_features.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To optimize the computational time, we create the new variable $\\mathcal{X} = \\left [ \\begin{matrix} 1 \\\\ x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\\\ \\end{matrix} \\right ]$ and we rewrite the equation $h(X)$ using the dot product.\n",
    "\n",
    "$ h(X) = \\sum_{i=1}^n \\beta_i x_i + \\beta_0 $\n",
    "\n",
    "$ h(X) = \\beta_n x_n + ... + \\beta_2 x_2 + \\beta_1 x_1 + \\beta_0 $\n",
    "\n",
    "$ h(X) = \\left [ \\begin{matrix} \\beta_0, \\beta_1, \\beta_2, ..., \\beta_n \\end{matrix} \\right ]^T \\left [ \\begin{matrix} 1 \\\\ x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\\\ \\end{matrix} \\right ] $\n",
    "\n",
    "$ h(X) = \\beta^T \\mathcal{X} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For calculating the $\\beta$ values using the samples $<x^{(i)}, y^{(i)} >$ we solve the following optimization problem: \n",
    "\n",
    "$ min_{\\beta} J(\\beta) = \\frac{1}{2} \\sum_{i=1}^M (h (x^{(i)}) - y^{(i)} ) ^2$\n",
    "\n",
    "It can be solved using Gradient Descent or Conjugate Gradient.\n",
    "\n",
    "$\\triangledown J (\\beta) = \\left [ \\begin{matrix}\n",
    "\\sum_i (h(x^{(i)})-y^{(i)})^2 \\\\ \n",
    "\\sum_i (h(x^{(i)})-y^{(i)})^2 (x^{(i)}_1)\\\\ \n",
    "\\sum_i (h(x^{(i)})-y^{(i)})^2 (x^{(i)}_2)\\\\ \n",
    "\\vdots \\\\ \n",
    "\\sum_i (h(x^{(i)})-y^{(i)})^2 (x^{(i)}_n) \\\\ \n",
    "\\end{matrix} \\right ]$\n",
    "\n",
    "$\\triangledown^2 J (\\beta) = \\begin{bmatrix}\n",
    "m & \\sum_i x_1^{(i)} & \\sum_i x_2^{(i)} & ... & \\sum_i x_n^{(i)}  \\\\ \n",
    "\\sum_i x_1^{(i)} & \\sum_i x_1^{(i)} x_1^{(i)}  & \\sum_i x_1^{(i)} x_2^{(i)} & ... & \\sum_i x_1^{(i)} x_n^{(i)} \\\\ \n",
    "\\sum_i x_2^{(i)} & \\sum_i x_2^{(i)} x_1^{(i)} & \\sum_i x_2^{(i)} x_2^{(i)}  & ... & \\sum_i x_2^{(i)} x_n^{(i)} \\\\ \n",
    "\\vdots  & \\vdots & \\vdots & \\ddots & \\vdots\\\\ \n",
    "\\sum_i x_n^{(i)} & \\sum_i x_n^{(i)} x_1^{(i)} & \\sum_i x_n^{(i)} x_2^{(i)}  & ... & \\sum_i x_n^{(i)} x_n^{(i)} \\\\  \n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables transformation\n",
    "\n",
    "Sometimes, a variable transformation can be very helpful to improve the regression.\n",
    "\n",
    "For example:\n",
    "<img src=\"images/6_variables_transformation.png\">\n",
    "\n",
    "The variables transformation can be performed using: multiplication, sum, trigonometric functions, etc.\n",
    "Important: The transformation needs to be done before to fit the linear regression model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling and feature normalization\n",
    "\n",
    "For improving the optimization process (Gradient Descent or Conjugate Gradient), a preprocessing of the data could be performed. \n",
    "\n",
    "__Feature scaling__ consists into transform the variables for having a range of 1.\n",
    "\n",
    "$ x = \\frac{x}{range} $\n",
    "\n",
    "__Feature normalization__ consists into transform the variables for having a normalized range (based on the normal distribution):\n",
    "\n",
    "$ x = \\frac{x-avr(x)}{ std(x)}$\n",
    "\n",
    "Where $avr(x)$ and $std(x)$ represents the average and the standard deviation of the feature $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Systems of linear equations\n",
    "\n",
    "Another way to solve the linear regression problem is using systems of linear equations:\n",
    "\n",
    "<img src=\"images/6_linear_systems.png\">\n",
    "\n",
    "$ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + + \\beta_n x_n $\n",
    "\n",
    "$ X \\beta = Y $\n",
    "\n",
    "$ \\begin{bmatrix}\n",
    "1 & 2104 & 5 & 1 & 45 \\\\ \n",
    "1 & 1416 & 3 & 2 & 40 \\\\ \n",
    "1 & 1534 & 3 & 2 & 30 \\\\ \n",
    "1 & 852 & 2 & 1 & 36 \\\\ \n",
    "1 & 1035 & 3 & 2 & 20 \\end{bmatrix} \\begin{bmatrix} \\beta_0 \\\\  \\beta_1 \\\\ \\beta_2 \\\\ ... \\\\ \\beta_n \\end{bmatrix} = \\begin{bmatrix} 460 \\\\  232 \\\\ 315 \\\\ 178 \\\\ 250 \\end{bmatrix} $\n",
    "\n",
    "The first approach could be:\n",
    "\n",
    "$ X \\beta = Y $\n",
    "\n",
    "$ X^{-1} X \\beta = X^{-1} Y $\n",
    "\n",
    "$ \\beta = X^{-1} Y $\n",
    "\n",
    "But $X$ is not square, so, it is not possible to calculate its inverse $X^{-1}$.\n",
    "\n",
    "The second approach is:\n",
    "\n",
    "$ X \\beta = Y $\n",
    "\n",
    "$ X^{T} X \\beta = X^{T} Y $\n",
    "\n",
    "$ (X^{T} X)^{-1} X^{T} X \\beta = (X^{T} X)^{-1} X^{T} Y $\n",
    "\n",
    "$ \\beta = (X^{T} X)^{-1} X^{T} Y  $\n",
    "\n",
    "It is a simple approach, buth the complexity of solving the linear system $O(n^3)$ is higher than Gradient Descent $O(n^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonlinear problems\n",
    "\n",
    "<img src=\"images/6_linear_nonlinear.PNG\">\n",
    "\n",
    "Sometimes the problems are nonlinear. In those cases, we can prove feature transformation **before** fitting the model.\n",
    "\n",
    "For example, if we have the features input features: $x$, $y$, the linear model could be:\n",
    "\n",
    "$h(x,y) = \\beta_0 + \\beta_1 x + \\beta_2 y $\n",
    "\n",
    "And, a nonlinear model could be:\n",
    "\n",
    "$h(x,y) = \\beta_0 + \\beta_1 x + \\beta_2 y + \\beta_3 x^2 + \\beta_4 y^4 + \\beta_5 xy$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
